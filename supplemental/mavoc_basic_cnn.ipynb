{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### W281 Final Project Supplemental Notebook\n",
        "### Basic CNN Approach to MAVOC Vehicle Classification\n",
        "\n",
        "This notebook tries to fit a simple convolutional neural network in order to compare the performance of a non-linear classifier that generates its own features through convolution versus the hand-engineered features in our main report notebook. The basic CNN performs outperforms our by-hand features and linear classifiers with an accuracy of 87% on the test dataset."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import copy\n",
        "import time\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import resample\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "\n",
        "from torchvision import datasets, models, transforms\n",
        "import torchvision.transforms.functional as TVF\n",
        "from torchvision.io import read_image"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure results are replicable\n",
        "seed = 281\n",
        "\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "np.random.seed(seed)\n",
        "random.seed(seed)\n",
        "torch.backends.cudnn.benchmark = False\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MavocDataset(Dataset):\n",
        "    \"\"\"MAVOC dataset containing pairs of EO + SAR images\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, eo_transform=None, sar_transform=None, composite_transform=None):\n",
        "        \"\"\"\n",
        "        Constructs a dataframe reference to each sample pair and label in the MAVOC \"training\" dataset\n",
        "        :param root_dir (string): path to the folder containing the class folders\n",
        "        :param transform (callable, optional): transforms to be applied on paired EO + SAR samples\n",
        "        \"\"\"\n",
        "\n",
        "        self.root_dir = root_dir\n",
        "        self.img_pairs = defaultdict(lambda: dict)\n",
        "\n",
        "        self.eo_transform = eo_transform\n",
        "        self.sar_transform = sar_transform\n",
        "        self.composite_transform = composite_transform\n",
        "\n",
        "        eo_prefix = \"EO\" # case sensitive!\n",
        "        sar_prefix = \"SAR\"\n",
        "        class_folders = os.listdir(self.root_dir)\n",
        "\n",
        "        # populate a dictionary with image_id number, the eo and sar file path, and class label. ignore hidden files.\n",
        "        for class_dir in class_folders:\n",
        "            if not class_dir.startswith('.'):\n",
        "                for file in os.listdir(os.path.join(self.root_dir, class_dir)):\n",
        "                    if not file.startswith('.'):\n",
        "                        id = int(re.findall(\"\\d+\", file)[0]) # grab the integer (image_id) in filename and use as key\n",
        "                        label = int(class_dir)\n",
        "                        img_path = os.path.join(self.root_dir,class_dir, file)\n",
        "\n",
        "                        if id in self.img_pairs.keys():\n",
        "                            if file.startswith(eo_prefix):\n",
        "                                self.img_pairs[id].update({\"eo_img\": img_path})\n",
        "                            if file.startswith(sar_prefix):\n",
        "                                self.img_pairs[id].update({\"sar_img\": img_path})\n",
        "                        else:\n",
        "                            if file.startswith(eo_prefix):\n",
        "                                self.img_pairs[id] = {\"eo_img\": img_path, \"sar_img\":None, \"label\":label}\n",
        "                            if file.startswith(sar_prefix):\n",
        "                                self.img_pairs[id] = {\"eo_img\": None,\"sar_img\": img_path, \"label\":label}\n",
        "\n",
        "        # convert the dict to a dataframe so that we can properly index into the dataset with __getitem__\n",
        "        self.img_labels_df = pd.DataFrame.from_dict(self.img_pairs, orient='index')\n",
        "        self.img_labels_df.reset_index(inplace=True)\n",
        "        self.img_labels_df = self.img_labels_df.rename(columns = {'index':'image_id'})\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        df = self.img_labels_df\n",
        "        eo_img_path = df.loc[df.index[idx], \"eo_img\"]\n",
        "\n",
        "        eo_image = read_image(eo_img_path) # reads jpeg or png into a 3d RGB or grayscale tensor (uint8 in [0,255])\n",
        "        eo_image = TVF.resize(eo_image, (32, 32))\n",
        "\n",
        "        label = df.loc[df.index[idx], \"label\"]\n",
        "\n",
        "        return eo_image, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels_df.index)"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MavocDataset(root_dir=\"train3\", eo_transform=None, sar_transform=None)"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a train-val-test (80-10-10) split with balanced class distribution (10% of each class in each of train/val/test splits). We decide to downsample to the minority class (class 7: flatbed truck)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "partition_df = dataset.img_labels_df.copy(deep=True)\n",
        "partition_df[\"idx\"] = partition_df.index\n",
        "downsample_amount = partition_df[\"label\"].value_counts().min() # 624\n",
        "\n",
        "classes_to_downsample = list(range(0,10))\n",
        "classes_to_downsample.pop(7)\n",
        "minority_class_df = partition_df[partition_df[\"label\"]==7]\n",
        "\n",
        "appended_data = [minority_class_df]\n",
        "\n",
        "for label in classes_to_downsample:\n",
        "    down_df = partition_df[partition_df[\"label\"]==label]\n",
        "    down_df = resample(down_df, replace=False, n_samples=downsample_amount, random_state=seed)\n",
        "    appended_data.append(down_df)\n",
        "\n",
        "samples_bal_df = pd.concat(appended_data)\n",
        "\n",
        "print(\"Downsampled train split class counts\")\n",
        "samples_bal_df[\"label\"].value_counts()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Downsampled train split class counts\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "7    624\n0    624\n1    624\n2    624\n3    624\n4    624\n5    624\n6    624\n8    624\n9    624\nName: label, dtype: int64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split into train/val/test, 80-10-10, and preserve the class distribution. Ensure class counts are as expected. Create pytorch dataset classes for each partition."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = samples_bal_df\n",
        "y = samples_bal_df[\"label\"].tolist()\n",
        "# 80% training\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=seed)\n",
        "\n",
        "# 10%/10% dev/test\n",
        "X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, stratify=y_test, test_size=0.5, random_state=seed)\n",
        "X_train[\"partition\"], X_dev[\"partition\"], X_test[\"partition\"]= \"train\", \"dev\", \"test\"\n",
        "\n",
        "partition_scheme_df = pd.concat([X_train, X_dev, X_test], axis=0)\n",
        "# partition_scheme_df.to_csv(\"mavoc_partition_scheme.csv\",index=False) # write this out for teammates\n",
        "\n",
        "print(f\"Train samples: {len(X_train.index)}\")\n",
        "print(f\"Validation samples: {len(X_dev.index)}\")\n",
        "print(f\"Test samples: {len(X_test.index)}\")\n",
        "\n",
        "train_indices = X_train[\"idx\"].tolist()\n",
        "val_indices = X_dev[\"idx\"].tolist()\n",
        "test_indices = X_test[\"idx\"].tolist()\n",
        "\n",
        "# Create pytorch subsets of the full dataset by the new stratified-partitioned indices\n",
        "train_dataset = Subset(dataset, train_indices)\n",
        "val_dataset = Subset(dataset, val_indices)\n",
        "test_dataset = Subset(dataset, test_indices)\n",
        "\n",
        "print(f\"Train set class counts \\n{X_train.label.value_counts()}\")\n",
        "print(f\"Dev set class counts \\n{X_dev.label.value_counts()}\")\n",
        "print(f\"Test set class counts \\n{X_test.label.value_counts()}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Train samples: 4992\nValidation samples: 624\nTest samples: 624\nTrain set class counts \n2    500\n3    500\n7    499\n5    499\n9    499\n0    499\n4    499\n1    499\n8    499\n6    499\nName: label, dtype: int64\nDev set class counts \n1    63\n4    63\n5    63\n7    63\n8    62\n6    62\n9    62\n3    62\n0    62\n2    62\nName: label, dtype: int64\nTest set class counts \n0    63\n8    63\n6    63\n9    63\n7    62\n5    62\n1    62\n3    62\n2    62\n4    62\nName: label, dtype: int64\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 4\n",
        "\n",
        "class_names = ('sedan', 'suv', 'pickup truck', 'van','box truck', 'motorcycle', 'flatbed truck','bus' , 'pickup truck with trailer',\n",
        "'flatbed truck with trailer')\n",
        "\n",
        "# Load individual partition\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "val_loader = DataLoader(dataset=val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "dataloaders = dict()\n",
        "dataloaders[\"train\"] = train_loader\n",
        "dataloaders[\"val\"] = val_loader\n",
        "dataloaders[\"test\"] = test_loader"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking the proper dimension sizes so we can properly specify the layer input and output dimensions."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(train_loader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "images=images.type(torch.float32)\n",
        "\n",
        "conv1 = nn.Conv2d(1,6,5)\n",
        "pool = nn.MaxPool2d(2,2)\n",
        "conv2 = nn.Conv2d(6, 16, 5)\n",
        "print(images.shape)\n",
        "x = conv1(images)\n",
        "print(x.shape)\n",
        "x = pool(x)\n",
        "print(x.shape)\n",
        "x=conv2(x)\n",
        "print(x.shape)\n",
        "x = pool(x)\n",
        "print(x.shape)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "torch.Size([4, 1, 32, 32])\ntorch.Size([4, 6, 28, 28])\ntorch.Size([4, 6, 14, 14])\ntorch.Size([4, 16, 10, 10])\ntorch.Size([4, 16, 5, 5])\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We created a simple convolutional neural network to see how this would perform relative to other linear classifiers. Our baseline CNN consisted of two convolutional layers with relu activation and the final feed-forward network consisted of two hidden layers plus final 10-node output layer.\n",
        "We trained for only 5 epochs using a learning rate of 0.001. We did not introduce dropout or any other enhancements to the network (including no augmentations up front to see how effective the convolutional layers were in generating local features on their own)."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters \n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5) # Color channels, output channel size, kernel size\n",
        "        self.pool = nn.MaxPool2d(2, 2) # kernel size 2x2, stride\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5) # input channel size==last output chnl size, output chnl, kernel sie\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # \n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, len(class_names))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # -> n, 3, 32, 32\n",
        "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 4, 4\n",
        "        x = x.view(-1, 16 * 5 * 5)            # -> n,  #flatten 3d tensor to 1d tensor\n",
        "        x = F.relu(self.fc1(x))               # -> n, 120\n",
        "        x = F.relu(self.fc2(x))               # -> n, 84\n",
        "        x = self.fc3(x)                       # -> n, 10\n",
        "        return x\n",
        "\n",
        "# Put everything on GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = ConvNet().to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "n_total_steps = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        # origin shape: [4, 1, 31, 31] = 4, 3, 1024\n",
        "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
        "        images = images.type(torch.float32)\n",
        "        labels = labels.type(torch.long)\n",
        "\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if (i+1) % 312 == 0:\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
        "\n",
        "print('Finished Training')\n",
        "PATH = './cnn.pth'\n",
        "torch.save(model.state_dict(), PATH)\n",
        "\n",
        "with torch.no_grad():\n",
        "    n_correct = 0\n",
        "    n_samples = 0\n",
        "    n_class_correct = [0 for i in range(10)]\n",
        "    n_class_samples = [0 for i in range(10)]\n",
        "    for images, labels in test_loader:\n",
        "        images = images.type(torch.float32)\n",
        "        labels = labels.type(torch.long)\n",
        "\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        # max returns (value ,index)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        n_samples += labels.size(0)\n",
        "        n_correct += (predicted == labels).sum().item()\n",
        "        \n",
        "        for i in range(BATCH_SIZE):\n",
        "            label = labels[i]\n",
        "            pred = predicted[i]\n",
        "            if (label == pred):\n",
        "                n_class_correct[label] += 1\n",
        "            n_class_samples[label] += 1\n",
        "\n",
        "    acc = 100.0 * n_correct / n_samples\n",
        "    print(f'Test Accuracy of the network: {acc} %')\n",
        "\n",
        "    for i in range(10):\n",
        "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
        "        print(f'Accuracy of {class_names[i]}: {acc} %')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch [1/5], Step [312/1248], Loss: 0.4337\nEpoch [1/5], Step [624/1248], Loss: 1.1835\nEpoch [1/5], Step [936/1248], Loss: 0.7446\nEpoch [1/5], Step [1248/1248], Loss: 0.2660\nEpoch [2/5], Step [312/1248], Loss: 0.8823\nEpoch [2/5], Step [624/1248], Loss: 0.1992\nEpoch [2/5], Step [936/1248], Loss: 0.1264\nEpoch [2/5], Step [1248/1248], Loss: 0.6384\nEpoch [3/5], Step [312/1248], Loss: 0.4437\nEpoch [3/5], Step [624/1248], Loss: 0.3215\nEpoch [3/5], Step [936/1248], Loss: 0.1676\nEpoch [3/5], Step [1248/1248], Loss: 1.1826\nEpoch [4/5], Step [312/1248], Loss: 0.4726\nEpoch [4/5], Step [624/1248], Loss: 0.6038\nEpoch [4/5], Step [936/1248], Loss: 0.3948\nEpoch [4/5], Step [1248/1248], Loss: 0.0874\nEpoch [5/5], Step [312/1248], Loss: 0.4270\nEpoch [5/5], Step [624/1248], Loss: 0.6105\nEpoch [5/5], Step [936/1248], Loss: 0.1099\nEpoch [5/5], Step [1248/1248], Loss: 0.0863\nFinished Training\nTest Accuracy of the network: 87.82051282051282 %\nAccuracy of sedan: 30.158730158730158 %\nAccuracy of suv: 82.25806451612904 %\nAccuracy of pickup truck: 98.38709677419355 %\nAccuracy of van: 87.09677419354838 %\nAccuracy of box truck: 100.0 %\nAccuracy of motorcycle: 95.16129032258064 %\nAccuracy of flatbed truck: 100.0 %\nAccuracy of bus: 88.70967741935483 %\nAccuracy of pickup truck with trailer: 98.41269841269842 %\nAccuracy of flatbed truck with trailer: 98.41269841269842 %\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The CNN struggles with predicting sedans in our test set, but performs highly on all other vehicle classes."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml-pt-tf",
      "language": "python",
      "display_name": "Python 3.8 - Pytorch and Tensorflow"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "python38-azureml-pt-tf"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}